{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report\n",
    "\n",
    "## Sourcing\n",
    "\n",
    "The WeRateDogs, data is split over three sources.\n",
    "    1. A twitter archive (CSV) file\n",
    "    2. Neural network predictions on the tweet images (online TSV file)\n",
    "    3. Tweet information (Twitter API)\n",
    "As such it was necessary to read in from all these locations. The local CSV file is easily read in by pandas. Using requests we can download the TSV file from the server and store it as a local TSV file, to be read in by pandas.\n",
    "\n",
    "The tweet information was collected through the Twitter API, and tweepy for python. Collecting the data for each tweet individually proved to be very slow and exceeded the rate limit for the API, instead using the ` GET statuses/lookup` we can pull the information in batches of 100 tweets. This reduced the time to download tweets from the predicted 20-30 minutes to around 13 seconds (24 group requests vs. 2356 individual requests). The API configuration was also set to use a JSON parser, so as to more easily store the tweets in the desired format, a JSON .txt file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing\n",
    "\n",
    "In assessing we looked over the data table by table both visually and programatically.\n",
    "\n",
    "Visual observations include items such as noting that text contains a shortened_url, or a dog with name \"O'Malley\" had his name incorrectly read as just \"O\".\n",
    "\n",
    "Programatic observations include items such as, missing tweet data, finding retweets and replies, and finding incorrect data formats.\n",
    "\n",
    "A summary of the observations actioned upon are listed below, split into groups for quality and tidiness issues.\n",
    "\n",
    "#### Quality\n",
    "\n",
    "##### ``archives`` table\n",
    "\n",
    "- Retweets and replies are included\n",
    "- Incorrect data format for `timestamp`\n",
    "- Incorrect rating data\n",
    "    - Decimal numerators are split\n",
    "    - Pulls from first fraction which may not be rating\n",
    "- Ratings should be normalised to score out of 10\n",
    "    - Some ratings are groups of dogs\n",
    "- Incorrect name data\n",
    "    - O'Malley\n",
    "    - Incorrect words taken as names (lowercase)\n",
    "- Missing data should be NaNs\n",
    "    - `dog type` and `name`\n",
    "- Tweet source unintuitive\n",
    "\n",
    "##### `predictions` table\n",
    "\n",
    "- No apparent quality issues\n",
    "\n",
    "##### `tweet_json` table\n",
    "\n",
    "- Missing tweets, 2346 vs 2355, owing to deleted tweets no longer available\n",
    "    - As seen these tweets are retweets/replies so will be dropped\n",
    "- Retweets and replies are included\n",
    "- Empty columns\n",
    "\n",
    "\n",
    "#### Tidiness\n",
    "\n",
    "- id_str is duplicate of id in `tweet_json`\n",
    "- Two variables in one column in `archive` `text`, shortened_url and text\n",
    "- One variable in four columns in `archive` table, dog type\n",
    "- Retweet count and favourite count from `tweet_json` should be attached to `archive`\n",
    "- Predictions from `predictions` should be attached to `archive`\n",
    "Cleaning\n",
    "In cleaning we first actioned removing incorrect data, such as retweets and replies, and removing missing data (namely empty columns), so as to not run into issues in the later cleaning steps.\n",
    "We then worked on the tidiness issues, such as splitting the shortened url from the text column and merging the four observations of dog type into one column. The other aspect worked on was to merge the desired columns from the tweet_json and predictions tables into the archive table.\n",
    "Moving on from this the remaining quality issues were addressed, generally in the order listed above. Where efficient the corrections were performed programatically, such as fixing the missing data, in other cases it was faster to manually correct issues that were one-off such as \"O'Malley.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "In cleaning we first actioned removing incorrect data, such as retweets and replies, and removing missing data (namely empty columns), so as to not run into issues in the later cleaning steps.\n",
    "\n",
    "We then worked on the tidiness issues, such as splitting the shortened url from the text column and merging the four observations of dog type into one column. The other aspect worked on was to merge the desired columns from the `tweet_json` and `predictions` tables into the `archive` table.\n",
    "\n",
    "Moving on from this the remaining quality issues were addressed, generally in the order listed above. Where efficient the corrections were performed programatically, such as fixing the missing data, in other cases it was faster to manually correct issues that were one-off such as \"O'Malley.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
